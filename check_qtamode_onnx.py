#é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã¨é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•åˆ¤å®šã—ã¦ONNXå¤‰æ›
import torch
import torch.nn as nn
import torch.quantization
import torch.onnx
from torch.quantization import convert
import types

def is_qat_model(model):
    """
    ãƒ¢ãƒ‡ãƒ«ãŒQATï¼ˆé‡å­åŒ–-aware trainingï¼‰æ¸ˆã¿ã‹ã©ã†ã‹ã‚’è‡ªå‹•åˆ¤å®š
    """
    for module in model.modules():
        # FakeQuantå±¤ã‚’æŒã£ã¦ã„ã‚Œã°QAT
        if hasattr(module, 'activation_post_process') or 'FakeQuantize' in str(type(module)):
            return True
    return False

def export_model_to_onnx(model, onnx_path, dummy_input_shape=(1, 3, 224, 224)):
    """
    ãƒ¢ãƒ‡ãƒ«ãŒQATæ¸ˆã¿ã‹floatã‹ã‚’è‡ªå‹•åˆ¤å®šã—ã€é©åˆ‡ã«ONNXå¤‰æ›ã‚’è¡Œã†

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ãƒ¼ãƒ‰æ¸ˆï¼‰
        onnx_path: ä¿å­˜å…ˆã®ONNXãƒ•ã‚¡ã‚¤ãƒ«å
        dummy_input_shape: ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®å½¢çŠ¶ï¼ˆä¾‹ï¼š224Ã—224ç”»åƒï¼‰
    """
    model.eval()
    dummy_input = torch.randn(dummy_input_shape)

    if is_qat_model(model):
        print("âœ… QATãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚convert() ã‚’é©ç”¨ã—ã¦ONNXå¤‰æ›ã—ã¾ã™ã€‚")
        model = convert(model)
    else:
        print("âœ… é€šå¸¸ã®floatãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ONNXå¤‰æ›ã—ã¾ã™ã€‚")

    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        input_names=["input"],
        output_names=["output"],
        opset_version=13,
        do_constant_folding=True
    )
    print(f"ğŸ‰ ONNXå¤‰æ›å®Œäº†ï¼š{onnx_path}")


from torchvision.models import efficientnet_b0

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨èª­ã¿è¾¼ã¿
model = efficientnet_b0(pretrained=False)
model.classifier[1] = nn.Linear(model.classifier[1].in_features, 288)

# é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆQATã§ã‚‚floatã§ã‚‚OKï¼‰
model.load_state_dict(torch.load("model.pth", map_location="cpu"))

# è‡ªå‹•åˆ¤å®š â†’ ONNXå¤‰æ›
export_model_to_onnx(model, "model_auto.onnx")

